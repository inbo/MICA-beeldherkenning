{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to classify new sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a tutorial that shows you how to classify new sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data\n",
    "* Camera trap images, in folders per deployment\n",
    "* Agouti export files: observations, assets and pickup-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the predefined configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the paths to the different folders. <br>\n",
    "If you have the same data structure as in this repository, you can use the predefined configuration file. If needed, you can change the paths according to you own folder structure.\n",
    "\n",
    "The following paths are defined:\n",
    "* **general_folder_path** : orginal camera trap images and the Agouti export files (assets, observations and pickup-setup)\n",
    "* **resized_folder_path** : resized camera trap images\n",
    "* **preprocessing_output_path** : preprocessing output\n",
    "* **crop_output_path** : cropped images (optional)\n",
    "* **draw_output_path** : regions of interest indicated on original camera trap images (optional)\n",
    "* **bottleneck_features_output_path** : extracted bottleneck features\n",
    "* **weight_path** : weights top model\n",
    "* **predictions_output_path** : predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\emma_cartuyvels\\appdata\\local\\continuum\\anaconda3\\envs\\cameratraps\\lib\\site-packages\\ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from yaml import load\n",
    "\n",
    "with open(\"config.yml\") as yaml_config:\n",
    "    config = load(yaml_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all folders exist and create folder if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for path in config:\n",
    "    if not os.path.exists(config[path]):\n",
    "        os.makedirs(config[path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: resize images\n",
    "The camera trap images are resized to 50% of their original size. This strongly decreases the computational time, while the performance remains the same.<br>\n",
    "\n",
    "Input: original camera trap images and Agouti export file (observations)<br>\n",
    "Output: resized camera trap images in a similar folder structure as the original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.resize_images import resize_images\n",
    "\n",
    "resize_images(config[\"general_folder_path\"], \n",
    "              config[\"resized_folder_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: preprocess images\n",
    "During the preprocessing, the regions of interest in the images are determined. All images of a sequence are used to construct a background image. Subsequently, the regions of interest in a camera trap image are determined by computing the difference between this background image and the camera trap image.\n",
    "\n",
    "Input: resized camera trap images and Agouti export files(observations + assets + pickupsetup)<br>\n",
    "Output: cvs-file containing the coordinates of the regions of interest in every camera trap image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boxes_sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-af869b049569>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[0mtotal_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtotal_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeployment\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m \u001b[0mboxes_sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'boxes_sequence' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageChops, ImageFilter\n",
    "from skimage import measure\n",
    "from skimage.filters import roberts\n",
    "from scipy import ndimage as ndi\n",
    "import cv2\n",
    "\n",
    "from preprocessing.def_functions import remove_dup_columns, black_border, standard_box, size_box, devide_box\n",
    "\n",
    "general_folder_path = '../data/raw'\n",
    "resized_folder_path = '../data/interim/resized'\n",
    "\n",
    "#Resized images\n",
    "RATIO = 0.5\n",
    "    \n",
    "#Import Agouti export files\n",
    "observations = pd.read_csv(os.path.join(general_folder_path, 'observations.csv'))\n",
    "assets = pd.read_csv(os.path.join(general_folder_path, 'assets.csv'), low_memory = False)\n",
    "setup = pd.read_csv(os.path.join(general_folder_path, 'pickup_setup.csv'))\n",
    "    \n",
    "#Combine annotations for sequences with multiple annotations\n",
    "list_columns = ['animalCount','animalTaxonID','animalIsDomesticated','animalScientificName','animalVernacularName','animalSex','animalAge', 'animalBehavior', 'deploymentID']\n",
    "observations_unique = pd.DataFrame()   \n",
    "for name in list_columns:\n",
    "    column_unique = observations.groupby('sequenceID')[name].apply(list).reset_index()\n",
    "    observations_unique = pd.concat([observations_unique, column_unique], axis=1)\n",
    "observations_unique = remove_dup_columns(observations_unique)\n",
    "    \n",
    "#Join annotations and pickup-setup data\n",
    "ann = assets.set_index('sequence').join(observations_unique.set_index('sequenceID'))\n",
    "ann.index.name = 'sequenceId'\n",
    "data = ann.merge(setup, on='sequenceId', how='left')\n",
    "data.reset_index(level=0, inplace=True)\n",
    "data.rename(columns={'index': 'sequenceID'}, inplace=True)\n",
    "data = data.drop([ 'id','type','originalFilename','destination','directory','exiftoolData','order',\n",
    "                  'createdAt','isFavourite','observations','isTimeLapse','deployment'], axis=1)\n",
    "        \n",
    "#Combine deploymentID of observation and pickup-setup into one column    \n",
    "data['deployment'] = \"\"\n",
    "for i, row in data.iterrows():\n",
    "    if isinstance(row.deploymentID, list):\n",
    "        row.deployment = row.deploymentID[0]\n",
    "    else:\n",
    "        row.deployment = row.deploymentId\n",
    "data = data.drop(['deploymentId', 'deploymentID'], axis=1)       \n",
    "\n",
    "#Combine annotations from observations and pickup-setup into one column     \n",
    "data['Annotation'] = \"\"\n",
    "for i, row in data.iterrows():\n",
    "    if row.isSetupPickup == 'WAAR':\n",
    "        row.Annotation = ['PickupSetup']\n",
    "    elif row.isBlank == 'WAAR':\n",
    "        row.Annotation = ['Blank']\n",
    "    elif isinstance(row.animalVernacularName, list):\n",
    "        row.Annotation = row.animalVernacularName\n",
    "\n",
    "#Remove row without annotation\n",
    "data['Annotation'].replace('', np.nan, inplace=True)\n",
    "data.dropna(subset=['Annotation'], inplace=True)\n",
    "\n",
    "#Adjuct notation of annotation for images with more than one annotation\n",
    "for i, row in data.iterrows():\n",
    "    if len(row.Annotation) > 1:\n",
    "        row.Annotation = list(set(row.Annotation))\n",
    "\n",
    "#Initialize lists and output dataframes    \n",
    "total_output = None\n",
    "boxes_output = None\n",
    "\n",
    "#Size standard box\n",
    "length_standard_box = 960*RATIO\n",
    "height_standard_box = 540*RATIO\n",
    "\n",
    "#Minimum number of pixels object and minimum number of pixels difference\n",
    "min_pixel_object = int(6000*RATIO**2)\n",
    "min_pixel_diff = int(500*RATIO**2)\n",
    "#Maximum number of pixels difference is calculated later, based on the image size.\n",
    "\n",
    "#Parameters for binary closing\n",
    "struct = np.ones((20,20)).astype(int)\n",
    "iter_closing = 5 #number of iterations\n",
    "\n",
    "#Loop over every deployment\n",
    "for folder in os.listdir(resized_folder_path):\n",
    "    imageFolderPath = os.path.join(resized_folder_path, folder)\n",
    "\n",
    "    #Check if it is a folder, not a file\n",
    "    if os.path.isdir(imageFolderPath):\n",
    "\n",
    "        annotations_deployment = []\n",
    "        image_names_sequences = []\n",
    "\n",
    "        data_deployment = data.loc[data['deployment'] == folder]\n",
    "        sequences = data_deployment.sequenceID.unique()\n",
    "\n",
    "        for seq in sequences:\n",
    "            image_names_sequences.append(data_deployment.loc[data_deployment['sequenceID'] == seq].filename.tolist())\n",
    "            annotations_deployment.append(data_deployment[data_deployment['sequenceID'] == seq].Annotation.iloc[0])\n",
    "\n",
    "        lengths = [len(i) for i in image_names_sequences]\n",
    "        deployment = pd.DataFrame({'ImagesNames': image_names_sequences,'SequenceID': sequences, 'Length':lengths, 'Annotation':annotations_deployment}) #Eventueel nog andere info toevoegen zoals aantal dieren.\n",
    "        deployment['box_standard'] = \"\"\n",
    "        deployment['box_small']= \"\"\n",
    "        deployment['deployment']= folder\n",
    "\n",
    "        #Loop over every sequence of the deployment\n",
    "        for i, row in enumerate(deployment.itertuples(), 1):\n",
    "\n",
    "            # Import images sequence\n",
    "            images_sequence = pd.DataFrame()\n",
    "            for img in row.ImagesNames:\n",
    "                if os.path.isfile(os.path.join(resized_folder_path,folder,img)):\n",
    "                    image = Image.open(os.path.join(resized_folder_path, folder, img))\n",
    "                    name = (image.filename).split('\\\\')[-1]\n",
    "                    images_sequence = images_sequence.append(pd.DataFrame([image, name]).T) \n",
    "            images_sequence.columns = ['Image', 'ImageName']\n",
    "\n",
    "            if len(images_sequence) == len(row.ImagesNames) and len(images_sequence) > 0: #All images available\n",
    "\n",
    "                #Import sequence\n",
    "                images_matrices = []\n",
    "                series = [] \n",
    "                box_list = []\n",
    "                box_list_small = []\n",
    "                image_type = []\n",
    "\n",
    "                #Import first image to determine the size of the black border for the whole sequence.\n",
    "                image_border = images_sequence.iloc[0]['Image']\n",
    "                border = black_border(image_border)\n",
    "\n",
    "                for rows in images_sequence.itertuples():\n",
    "                    image = rows.Image\n",
    "                    image = image.crop(border)\n",
    "\n",
    "                    #Check if image is a greyscale image\n",
    "                    if len(set(image.getpixel((length_standard_box,length_standard_box)))) == 1 & len(set(image.getpixel((height_standard_box,height_standard_box)))) == 1: \n",
    "                        image = image.convert('L')\n",
    "                        image_type.append('grey')\n",
    "                    else:\n",
    "                        image_type.append('color')\n",
    "\n",
    "                    series.append(image)\n",
    "                    images_matrices.append(np.asarray(image))\n",
    "\n",
    "                #Valid sequence? (Remove control images)\n",
    "                dim_images = [len(k.shape) for k in images_matrices]\n",
    "                if row.Length >= 10 and len(set(dim_images)) == 1:\n",
    "\n",
    "                    #Calculate the median value of every pixel to determine the background\n",
    "                    dim = images_matrices[0].ndim\n",
    "                    image_stack = np.concatenate([im[..., None] for im in images_matrices], axis=dim)\n",
    "                    median_array = np.median(image_stack, axis=dim)\n",
    "                    median_image = Image.fromarray(median_array.astype('uint8'))    \n",
    "\n",
    "                    #Image size\n",
    "                    image_length = series[0].size[0]\n",
    "                    image_height = series[0].size[1]\n",
    "\n",
    "                    #Maximum number of pixels difference\n",
    "                    max_pixel_diff = image_length*image_height*0.6\n",
    "\n",
    "                    #Select objects\n",
    "                    for img in series:\n",
    "\n",
    "                        #Difference with background\n",
    "                        diff = ImageChops.difference(median_image, img).convert('L')\n",
    "\n",
    "                        #MinFilter\n",
    "                        filter = diff.filter(ImageFilter.MinFilter(size=9))\n",
    "\n",
    "                        #Number of pixels that are different\n",
    "                        pixels_filter = cv2.countNonZero(np.asarray(filter))\n",
    "                        box_filter = filter.getbbox()\n",
    "\n",
    "                        #No (significant) difference with background\n",
    "                        if not isinstance(box_filter, tuple) or pixels_filter < min_pixel_diff :\n",
    "                            length_box_filter = 0\n",
    "                            height_box_filter = 0\n",
    "                            box_filter = ()\n",
    "\n",
    "                            box_list.append(box_filter)\n",
    "                            box_list_small.append(box_filter)\n",
    "\n",
    "                        #To much difference with background\n",
    "                        elif pixels_filter > max_pixel_diff:\n",
    "                            box_object_list_small = ()\n",
    "                            box_object_list = devide_box(img.getbbox(), length_standard_box, height_standard_box, image_length, image_height)\n",
    "\n",
    "                            box_list.append(box_object_list)\n",
    "                            box_list_small.append(box_object_list_small)\n",
    "\n",
    "                        else: \n",
    "                            length_box_filter = size_box(box_filter)[0]\n",
    "                            height_box_filter = size_box(box_filter)[1]\n",
    "\n",
    "                            #Box after filtering is smaller than standard box\n",
    "                            if length_box_filter < length_standard_box and height_box_filter < height_standard_box:\n",
    "                                box = standard_box(box_filter, length_standard_box, height_standard_box, image_length, image_height)\n",
    "                                box_list.append(box)\n",
    "                                box_list_small.append(box_filter)\n",
    "\n",
    "                            #Box after filtering is larger than standard box\n",
    "                            else:\n",
    "                                #Edge detection\n",
    "                                edge = roberts(filter)\n",
    "\n",
    "                                #MinFilter after edge detection\n",
    "                                edge = (edge != 0).astype(int)\n",
    "                                edge = Image.fromarray(edge.astype('uint8')).filter(ImageFilter.MinFilter(size=3))\n",
    "                                edge = Image.fromarray(np.asarray(edge).astype('uint8')).filter(ImageFilter.MinFilter(size=3))\n",
    "\n",
    "                                #Binary closing\n",
    "                                closing = ndi.binary_closing(edge, structure=struct, iterations=iter_closing, output=None, origin=0)\n",
    "\n",
    "                                #Connected component labeling\n",
    "                                connect = measure.label(closing, neighbors=8, background=0, return_num=True)\n",
    "                                counts = np.bincount(connect[0].flatten())\n",
    "\n",
    "                                #Box after connected component labeling\n",
    "                                box = Image.fromarray(closing.astype('uint8')).getbbox()\n",
    "                                if not isinstance(box, tuple):\n",
    "                                    length_box = 0\n",
    "                                    height_box = 0\n",
    "                                    box = ()      \n",
    "                                else:\n",
    "                                    length_box = size_box(box)[0]\n",
    "                                    height_box = size_box(box)[1]\n",
    "\n",
    "                                #Box not empty\n",
    "                                if length_box != 0:\n",
    "\n",
    "                                    #Box after connected component labeling is larger than standard box\n",
    "                                    if length_box > length_standard_box or height_box > height_standard_box:\n",
    "\n",
    "                                        #Boxes around objects\n",
    "                                        box_object_list = []\n",
    "                                        box_object_list_small = []\n",
    "\n",
    "                                        for a in range(1, (connect[1])+1):\n",
    "\n",
    "                                            if counts[a] > min_pixel_object:\n",
    "\n",
    "                                                box_object = Image.fromarray((connect[0]==a).astype('uint8')).getbbox()\n",
    "                                                box_object_list_small.append(box_object)\n",
    "\n",
    "                                                length_box_object = size_box(box_object)[0]\n",
    "                                                height_box_object = size_box(box_object)[1]\n",
    "\n",
    "                                                #Box around object bigger than standard box\n",
    "                                                if length_box_object > length_standard_box or height_box_object > height_standard_box:\n",
    "\n",
    "                                                    boxes = devide_box(box_object, length_standard_box, height_standard_box, image_length, image_height)\n",
    "                                                    box_object_list += boxes\n",
    "\n",
    "                                                #Box around object is smaller than standard box\n",
    "                                                else:\n",
    "                                                    box_object = standard_box(box_object,length_standard_box,height_standard_box, image_length, image_height)\n",
    "                                                    box_object_list.append(box_object)\n",
    "\n",
    "                                        if not box_object_list:\n",
    "                                            box_object_list = ()\n",
    "                                            box_object_list_small = ()\n",
    "                                        box_list.append(box_object_list)\n",
    "                                        box_list_small.append(box_object_list_small)\n",
    "\n",
    "\n",
    "                                    #Box after connected component labeling is smaller than standard box\n",
    "                                    else:\n",
    "                                        box_list_small.append(box)\n",
    "                                        box = standard_box(box,length_standard_box,height_standard_box, image_length, image_height)\n",
    "                                        box_list.append(box)\n",
    "\n",
    "                                #Empty box\n",
    "                                else: \n",
    "                                    box_list.append(box)\n",
    "                                    box_list_small.append(box)\n",
    "\n",
    "\n",
    "                    #Save sequence preprocessing data\n",
    "                    if all(isinstance(x, (tuple)) for x in box_list):\n",
    "                        box_list = [[elem] for elem in box_list]\n",
    "                        df_standard = pd.DataFrame([box_list]).transpose()\n",
    "                    else:\n",
    "                        lengths = []\n",
    "                        for l in range(len(box_list)):\n",
    "                            item = box_list[l]\n",
    "                            if isinstance(item, (tuple)):\n",
    "                                box_list[l] = [item]       \n",
    "                            lengths.append(len(box_list[l]))\n",
    "\n",
    "                        if all(lengths[0] == items for items in lengths):\n",
    "                            df_standard = pd.DataFrame([box_list]).transpose()\n",
    "                        else:\n",
    "                            df_standard = pd.DataFrame(np.array(box_list).reshape(len(row.ImagesNames),-1))\n",
    "\n",
    "\n",
    "                    if all(isinstance(x, (tuple)) for x in box_list_small):\n",
    "                        box_list_small = [[elem] for elem in box_list_small]\n",
    "                        df_small = pd.DataFrame([box_list_small]).transpose()\n",
    "\n",
    "                    else:\n",
    "                        lengths = []\n",
    "                        for l in range(len(box_list_small)):\n",
    "                            item_small = box_list_small[l]\n",
    "                            if isinstance(item_small, (tuple)):\n",
    "                                box_list_small[l] = [item_small]        \n",
    "                            lengths.append(len(box_list_small[l]))\n",
    "\n",
    "                        if all(lengths[0] == items for items in lengths):\n",
    "                            df_small = pd.DataFrame([box_list_small]).transpose()\n",
    "                        else:\n",
    "                            df_small = pd.DataFrame(np.array(box_list_small).reshape(len(row.ImagesNames),-1))\n",
    "\n",
    "                    boxes_sequence = pd.concat([df_standard,df_small], axis=1)\n",
    "                    boxes_sequence.columns = ['box_standard', 'box_small']\n",
    "                    boxes_sequence['deployment'] = folder\n",
    "                    boxes_sequence['sequence'] = row.SequenceID\n",
    "                    boxes_sequence['annotation'] = \"\"\n",
    "                    for seq_index, seq_row in boxes_sequence.iterrows():\n",
    "                        seq_row.annotation = row.Annotation\n",
    "                    boxes_sequence['image_name'] = pd.DataFrame(row.ImagesNames)\n",
    "                    boxes_sequence['image_type'] = pd.DataFrame(image_type)\n",
    "\n",
    "                    if boxes_output is None:\n",
    "                        boxes_output = boxes_sequence\n",
    "                    else:\n",
    "                        boxes_output = pd.concat([boxes_output, boxes_sequence], axis = 0)\n",
    "\n",
    "                    #Save smallest box and standard box\n",
    "                    deployment.set_value(deployment.index[i-1], 'box_standard', box_list)\n",
    "                    deployment.set_value(deployment.index[i-1], 'box_small', box_list_small)\n",
    "\n",
    "        #Save deployment\n",
    "        if total_output is None:\n",
    "            total_output = deployment\n",
    "        else:\n",
    "            total_output = pd.concat([total_output, deployment])\n",
    "            \n",
    "boxes_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b412e9f0b000>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m preprocessing(config[\"general_folder_path\"], \n\u001b[0;32m      4\u001b[0m               \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"resized_folder_path\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m               config[\"preprocessing_output_path\"])\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\MICA-beeldherkenning\\CameravallenCode-master\\src\\preprocessing\\preprocessing.py\u001b[0m in \u001b[0;36mpreprocessing\u001b[1;34m(general_folder_path, resized_folder_path, preprocessing_output_path)\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;31m#Every box => Image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[0mboxes_single\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mboxes_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbox_standard\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "from preprocessing.preprocessing import preprocessing\n",
    "\n",
    "preprocessing(config[\"general_folder_path\"], \n",
    "              config[\"resized_folder_path\"], \n",
    "              config[\"preprocessing_output_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: crop images or indicate regions of interest\n",
    "We can crop the camera trap images or indicate the regions of interest on the camera trap images to see the result of the preprocessing. <br>\n",
    "This step is optional and not required to classify the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.crop_images import crop_images\n",
    "from preprocessing.draw_boxes import draw_boxes\n",
    "\n",
    "crop_images(config[\"preprocessing_output_path\"], config[\"resized_folder_path\"], config[\"crop_output_path\"])\n",
    "draw_boxes(config[\"preprocessing_output_path\"], config[\"resized_folder_path\"], config[\"draw_output_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import image\n",
    "\n",
    "# Show a camera trap image with indication of the regions of interest\n",
    "deployment = os.listdir(config[\"draw_output_path\"])[0]\n",
    "image_name = os.listdir(deployment)[0]\n",
    "image = Image.open(os.path.join(config[\"draw_output_path\"],deployment, image_name))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: extract bottleneck features\n",
    "\n",
    "The pretrained convolutional neural network ResNet50 is used to convert the images to bottleneck features.\n",
    "\n",
    "Input: resized camera trap images and preprocessing output containing the coordinates of the boxes <br>\n",
    "Output: bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.resnet50_bottleneck_features_predict import extract_bottleneck_features\n",
    "\n",
    "extract_bottleneck_features(config[\"preprocessing_output_path\"], \n",
    "                            config[\"bottleneck_features_output_path\"], \n",
    "                            config[\"resized_folder_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : run top model to classify the new images\n",
    "\n",
    "The extracted bottleneck features are fed to the new top model to predict the labels of the new images.\n",
    "\n",
    "Input: extracted bottleneck features <br>\n",
    "Ouput: probabilities of the output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.resnet50_hierarchical_bottleneck_predict import hierarchical_bottleneck_predict\n",
    "\n",
    "hierarchical_bottleneck_predict(config[\"bottleneck_features_output_path\"], \n",
    "                                config[\"weight_path\"], \n",
    "                                config[\"predictions_output_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : convert output probabilities to hierarchical classification\n",
    "\n",
    "The predictions of the individual images are aggregated to a hierarchical prediction for every sequence.\n",
    "\n",
    "Input: probabilities of the output classes for the individual images <br>\n",
    "Output: hierarchical classification of the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.hierachical_processing_predictions import hierarchical_predictions_sequences\n",
    "\n",
    "hierarchical_predictions_sequences( config[\"predictions_output_path\"], \n",
    "                                   config[\"bottleneck_features_output_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the file containing the hierarchical predictions to see the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions = pd.read_csv(os.path.join(config[\"predictions_output_path\"],'hierarchical_predictions_sequences.csv'), sep = ';')\n",
    "predictions.drop(['level_1_p','level_2_p','level_3_p','level_4_p','level_5_p'], axis=1, inplace=True)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Object localization\n",
    "\n",
    "The class activation maps can be used to localize the objects in the cropped camera trap images. <br>\n",
    "Since this step uses the cropped images, make sure to first run the optional cropping step above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.resnet_cam import object_localization\n",
    "\n",
    "img_path = os.path.join(config[\"crop_output_path\"], os.listdir(config[\"crop_output_path\"])[0])\n",
    "object_localization(img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cameratraps",
   "language": "python",
   "name": "cameratraps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
